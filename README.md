# Give-Me-Some-Credit-Kaggle-Competition

Проект основан на участии в соревновании Kaggle: [Give Me Some Credit](https://www.kaggle.com/c/GiveMeSomeCredit). Цель — предсказать, случится ли серьёзная просрочка по кредиту у клиента в течение следующих двух лет.

---

##  Цель исследования

Построить модель, которая предсказывает вероятность дефолта (SeriousDelinquency2yrs) на основе данных о платёжеспособности клиента.

---

##  Используемые данные

Данные содержат следующие признаки:

- **Числовые признаки** (`numerical`): `MonthlyIncome`, `logOpenCreditLinesCount`, `DebtToIncomeRatio`, `CreditUtilizationRatio`
- **Категориальные признаки** (`categorical`): `Age_group`, `30-59_D`, `60-89_D`, `90+_D`, `RealEstateLoansCount`
- **Бинарные признаки** (`binory`): `DependentsCount`

Целевая переменная: `SeriousDelinquency2yrs` (0 — нет, 1 — была просрочка).

---

##  Этапы работы

1. **Загрузка и очистка данных**
   - Обработка пропусков
   - Создание новых признаков (`logOpenCreditLinesCount`, `Age_group`)
   - Преобразование типов (`category` для категориальных)

2. **EDA (разведочный анализ)**
   - Визуализация распределений
   - Проверка дисбаланса классов
   - Корреляционный анализ

3. **Базовая модель**
   - Логистическая регрессия (`LogisticRegression`)
   - Метрики: `F1-score`, `ROC AUC`

4. **Градиентный бустинг**
   - Использование моделей:
     - `CatBoostClassifier`
     - `LGBMClassifier`
   - Ручная настройка параметров
   - Кросс-валидация (`StratifiedKFold`, 5 фолдов)
   - Оценка метрик (F1, ROC AUC)

5. **Выводы по моделям**
   - Модели бустинга показали значительно более высокое качество по сравнению с логистической регрессией
   - `CatBoost` и `LightGBM` хорошо работают с категориальными признаками
   - Использование кросс-валидации позволило корректно оценить обобщающую способность

---

##  Метрики (по кросс-валидации)

| Модель         | ROC AUC (avg) | F1-score (avg) |
|----------------|---------------|----------------|
| LogisticRegression |     ~0.72     |     ~0.23      |
| CatBoostClassifier |     ~0.88     |     ~0.36      |
| LGBMClassifier      |     ~0.87     |     ~0.35      |

---

##  Общие выводы

- Лучшие результаты достигнуты с использованием моделей градиентного бустинга
- Предобработка данных и корректное указание категориальных признаков — критически важны
- Кросс-валидация необходима для стабильной оценки
- Модель готова для финального предсказания и создания submission-файла на Kaggle

---

##  Используемые библиотеки

- `pandas`, `numpy`
- `scikit-learn`
- `lightgbm`
- `catboost`
- `matplotlib`, `seaborn`

---
